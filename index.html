<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>ECE 4760: Happy Little Mixer</title>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="css/header-features.css" rel="stylesheet">


</head>

<body id="page-top">

  <!-- Navigation 
<nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
    <div class="container">
      <a class="navbar-brand" href="#">ECE 5725: COOPY</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="index.html">Home
              <span class="sr-only">(current)</span>
            </a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#intro">Introduction</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#design">Design/Testing</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#results">Results</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#future">Future Work</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#references">References</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>-->

  <!-- Carousel Header -->
  <header>
    <div id="carouselExampleIndicators" class="carousel slide" data-ride="carousel">
      <ol class="carousel-indicators">
        <li data-target="#carouselExampleIndicators" data-slide-to="0" class="active"></li>
        <li data-target="#carouselExampleIndicators" data-slide-to="1"></li>
        <li data-target="#carouselExampleIndicators" data-slide-to="2"></li>
      </ol>
      <div class="carousel-inner" role="listbox">
        <!-- Slide One -->
        <div class="carousel-item active" style="background-image: url(images/fullsystem.jpg)">
          <div class="carousel-caption d-none d-md-block">
            <!--<h3 class="display-4">First Slide</h3>
            <p class="lead">This is a description for the first slide.</p>-->
          </div>
        </div>
        <!-- Slide Two -->
        <div class="carousel-item" style="background-image: url(images/hands.png)">
        </div>
        <!-- Slide Three -->
        <div class="carousel-item" style="background-image: url(images/swatches.jpg)"></div>
      </div>
    </div>
    <a class="carousel-control-prev" href="#carouselExampleIndicators" role="button" data-slide="prev">
      <span class="carousel-control-prev-icon" aria-hidden="true"></span>
      <span class="sr-only">Previous</span>
    </a>
    <a class="carousel-control-next" href="#carouselExampleIndicators" role="button" data-slide="next">
      <span class="carousel-control-next-icon" aria-hidden="true"></span>
      <span class="sr-only">Next</span>
    </a>
    </div>
  </header>

  <header class="bg-light">
    <div class="container text-center">
      <h7>Henri Clarke (hxc2), Michael Rivera (mr858), Priya Kattappurath (psk92)<br>
        ECE 4760 Final Project<br>
        Fall 2019 (12/13/19)</h7>
      <p class="lead"></p>
    </div>
  </header>

  <section id="introduction">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">

          <p class="lead">
            <h2>Introduction</h2>
            The Happy Little Mixer is an automatic ink mixer which accepts
            either a hex user input and creates that color by measuring out
            cyan, magenta, yellow, and black (CMYK) ink. It includes open-loop
            feedback using a color sensor to correct the generated color, making
            it as accurate as possible.

          </p>
        </div>
      </div>
    </div>
  </section>

  <section id="design">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <p class="lead">
            <h2>High Level Design</h2>

            We wanted to create something that would spark joy in a user and
            could be fun and interactive. Ideally, an ink mixer could be used
            to help teach young artists how to mix colors, as well as the
            principles of hue and saturation. It could also make the process of
            creating specific colors easier, particularly for colorblind
            artists. We called it a “happy little mixer” because we were
            inspired by the well known phrase, “happy little trees,” used by
            Bob Ross on his show “Joy of Painting.”

            <br><br>

            <h4>Logical Structure</h4>

            <img src="images/logicalstructure.png" alt="System diagram" style="width:600px;">

            <br>
            Above is a general flow diagram of the system we provided for our
            demo. The system will initially waits for a serial hex input from
            the user and then convert it to CMYK values. With these values, we
            determine how long each servo should dispense its corresponding
            color. The system then waits for the user to indicate over serial
            whether to make a color adjustment or return to the beginning. If
            the user wants to make an adjustment, we prompt the to hold a
            sample up to the color sensor for a more accurate reading and then
            wait for the user to indicate they are ready. Once ready, the
            system requests color data from an arduino attached to the color
            sensor. The system then performs a similar conversion with the
            color sensor data and compares the observed CMYK values with the
            expected values. The user is prompted again if they would like to
            make a color correction. If yes, then the system will attempt to
            modify the saturation of the appropriate channels.

            <br><br>

            <h4>Hardware and Software Tradeoffs</h4>

            Initially, our plan was to create a paint mixer, but the viscosity
            of the paint introduced mechanical issues that we couldn’t figure
            out in the given time, with the given budget. Consequently, we
            opted to use printer ink mixed with water to create dyes.
            Although this solved our initial budget issues, we encountered
            issues with secure closing mechanisms and leaks. We were also
            unable to implement closed loop feedback, because the color of dye
            in liquid form greatly differs to its color on paper.

            <br><br>

            On the software end, to reduce the complexity of our code, we
            decided to run our servos off a singular timer with four different
            output compare units using it as a source. This required that the
            corresponding duty cycle for each servo be calibrated not only for
            that specific servo, but also relative to the other servos due to
            their unique 20ms hold-off requirement.

            <br><br>

            <h4>Background Information</h4>
            We used some basic arithmetic in the use for color conversion --
            from hex, to RGB, to CMYK -- and scaling those results for output.
            We also used basic arithmetic for error calculations for the
            feedback system.
            <br><br>
            It should be noted that for CMYK colors, only two of the CMY
            values are ever nonzero at any time unless you are working with a
            grayscale value. Additionally, we had to modify the provided
            protothreads header file, pt_1_3_3.h, as it had not defined the
            auxiliary term buffer, which we needed to receive values from the
            Arduino.
            <br><br>
            To our knowledge, there are no patents for a similar product,
            although there are programs that do
            <a href="https://trycolors.com/">virtual color mixing</a>.
            We did not reference these in our development process.
            <br><br>
            This project complies with industry standards.

          </p>
        </div>
      </div>
    </div>
  </section>

  <section id="mechanical">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <p class="lead">
            <h2>Mechanical Design</h2>

            Our initial mechanical design differed from our final design.
            This was because we changed our mind in deciding to use servos to
            clamp the surgical tubing after we were in the process of printing
            the already designed structure, and we decided to prioritize the
            technical aspects of our project rather than spending time redoing
            the external structure. We worked modularly and only assembled the
            entire project in the final week.
            <br><br>
            When we began to design the outer casing for our color mixer,
            we knew we needed a place for the CMYK inks, servos, and places to
            route the servo wires and the tubing. Keeping these ideas in mind,
            we created a 3D model of our proposed casing, which included levels
            to hold the ink and servos, holes to route the tubing, and slits for
            wires. In order to print the casing in a reasonable amount of time,
            we split the design into three sections.
            <center>
              <img src="images/casing.png" alt="Casing" style="width:600px;">
              <br>
              Screenshot of casing, designed with Fusion.</center>

            <br><br>
            This proved very useful, as
            we were able to modify our original design by rearranging the 3 pieces
            to be better suited to hold the servos and tubing. We also added wooden
            spacers, which gave us more vertical space so the tubing could fully
            extend. Also, instead of using the premade holes in the casing only
            for tubing, we used them to hold the servos, and later drilled extra
            holes for the tubing.
            <br><br>

            We cut our [length] of tubing into four pieces. First, we tested to
            ensure that the servos were capable of pinching the tubing shut.
            After we determined the correct calibration, we drilled holes into
            the 3D print to make space for the tubing and added the servos.
            <br><br>
            <center>
              <img src="images/newservotop.jpg" alt="Top" style="width:400px;">
              <br>
              <img src="images/newservobottom.jpg" alt="bottom" style="width:400px;">
              <br>
              Servos and tubing newly added, top and bottom.</center>
            <br><br>
            We added wooden spacers to allow the servos to move without hitting
            the wall of the structure, then hot glued the next layer on. We
            threaded the tubing through the top holes and used hot glue to secure
            them in place and create a waterproof layer.
            <br><br>
            <center>
              <img src="images/spacers.jpg" alt="Spacers" style="width:600px;">
              <br>
              Spacers added to elevate top level away from servos.</center>
            <br><br>
            Next, we drilled holes into the bottom of the plastic bottles to be
            slightly wider than the tubing. We then placed hot glue around the
            tubing and slipping the plastic bottle on, then added additional hot
            glue to create a water seal. We tested this with water to ensure that
            there was no leakage, and that the servos were still capable of
            clamping the tubing properly.

            <br><br>
            <center>
              <img src="images/bottles.jpg" alt="Bottles" style="width:400px;">
              <br>
              Reservoirs of ink sealed off using hot glue.</center>

            <br><br>
            The servos were too low for a cup to fit under, so we sawed some
            stilts from a scrap block of wood and glued them in for the
            appropriate height. We also trimmed the tubing to ensure that they
            wouldn’t get contaminated in the already mixed dye.
            <br><br>
            <center>
              <img src="images/stilts.jpg" alt="stilts" style="width:400px;">
              <br>
              Stilts added to elevate everything from bottom level. Support in the back not shown.</center>
            <br><br>
            In retrospect, we could have done it more elegantly, but we did not
            lay out our mechanical rigorously enough at the beginning and were
            too pressed for time to redo it. In addition, the size of our 3D
            printed pieces were limited by the upper limits of the 3D printer.
            Despite these challenges, our final product was functional.


          </p>
        </div>
      </div>
    </div>
  </section>

  <section id="hardware">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <p class="lead">
            <h2>Hardware Design</h2>
            Our overall circuit design is as follows:
            <center>
              <img src="images/circuitdiagram.png" alt="Casing" style="width:600px;">
              <br>
              Overall circuit diagram.</center>

            <h4>Small Board</h4>
            To mount the PIC32 for this project, we utilized a Small Board
            designed by Sean Carroll. The Small Board is a breakout carrier for
            the PIC32. Our motivation for using this was primarily budget
            constraints while maintaining the familiarity of a breakout board
            similar to the one we had used for the previous labs in this class.
            <br><br>
            <center>
              <img src="images/smallboard.png" alt="Casing" style="width:600px;">
              <br>
              Sean Carroll's small board.</center>

            <br><br>

            More on the small board can be found on the <a
              href="https://people.ece.cornell.edu/land/courses/ece4760/PIC32/target_board.html">class webpage.</a>

            <br><br>

            <h4>Parallax Standard Servos</h4>
            Our initial plans for this project were to either use motor
            controlled syringes or peristaltic pumps to dispense paint.
            However, due to mechanical complexity and budget constraints
            respectively, we settle on a gravity based system with surgical
            tubing and servos to clamp them shut. We used standard servos
            ecause they were readily available at the time courtesy of Joe
            Skovira.
            <br>
            <br>

            <center>
              <img src="images/servo_wiring.png" alt="Casing" style="width:600px;">
              <br>
              Servo wiring, from <a
                href="https://www.parallax.com/sites/default/files/downloads/900-00005-Standard-Servo-Product-Documentation-v2.2.pdf">datasheet.</a>
            </center>
            <center>
              <img src="images/servo_timing.png" alt="Casing" style="width:600px;">
              <br>
              Servo timing, from <a
                href="https://www.parallax.com/sites/default/files/downloads/900-00005-Standard-Servo-Product-Documentation-v2.2.pdf">datasheet</a>.
            </center>


            <br><br>
            Above are the wiring and timing schemes for the servos from their
            datasheet. The servos have a power requirement of 4 to 6V and an
            I/O input range of 3.3 to 5V. The required PWM timing is that pulse
            widths should be between 0.75ms and 2.25ms, and pulses should be
            separated by 20ms.

            <br><br>
            <h4>Color Sensor + Arduino</h4>

            We used the color sensor <a href="https://www.adafruit.com/product/1334">TCS34725.</a>
            <br><br>
            Other than the normal wiring of the Color Sensor (which is in the
            circuit diagram for the project), we also needed to add additional
            circuitry between the Arduino transmit pin (pin 1) and the receive
            pin on the PIC32 (PIN RB13). This is because the Arduino runs on 5V
            power, and the PIC32 runs on 3.3V. Therefore, we needed to add a low
            pass filter between these two pins, in order to make sure the PIC32
            wasn’t overloaded.

          </p>
        </div>
      </div>
    </div>
  </section>

  <section id="software">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <p class="lead">
            <h2>Software Design</h2>
            <h4>CMYK Conversion</h4>
            RGB values are colors defined by the amounts of red, green, and blue
            light they contain. CMYK values are colors defined by the amounts of
            cyan, magenta, yellow, and black pigment they contain. We used the
            CMY primary colors because we were mixing pigment, and we decided to
            allow the user to pick a HEX value -- which are simply RGB values
            strung together in hexadecimal to make a 6-digit string -- because
            it would be an easier format for a user to enter. Thus, we needed to
            convert from HEX to CMYK.
            <br><br>

            <center>
              <img src="images/cmyk.png" alt="Casing" style="width:300px;">
              <br>
              <a href="https://99designs.com/blog/tips/correct-file-formats-rgb-and-cmyk/">Source</a></center>

            <br><br>
            Very coincidentally, someone had already written a <a
              href="http://www.javascripter.net/faq/hex2cmyk.htm">HEX to CMYK
              conversion in JavaScript. </a>We used its logic to write our own HEX
            to CMYK conversion in C. Our conversion took a HEX code as character
            array of length 6, pulled the RGB values from it, then converted
            those according to the referenced code.


            <br><br>
            <h4>Color Sensing</h4>
            The TCS34725 came with its own arduino source code and library.
            Here is a link to the <a href="https://github.com/adafruit/Adafruit_TCS34725"> original github repo.</a>
            In this code, we took the example tcs34725.ino, renamed it COLORSENSOR.ino, and modified it to fit our
            needs. <br><br>
            The original code got the raw RGB values and also some extra
            calculations such as color temperature, which we removed.
            The color sensor raw outputs range from 0-65535. In order to make
            them usable RGB values, we divided the raw value by 256 to scale it
            to 0-255. However, this number usually ended up fairly small within
            the 0-255 range (generally under 30), so multiplied it by 10.
            Sometimes the “multiply by 10” put the value above 255.
            In this case, we did a second scale to the RGB values so the
            highest one is 255 and the rest are scaled down accordingly.
            <br><br>
            Once we had usable RGB values between 0-255, we converted the values
            float to integer. This integer was compared to the previous RGB
            values, which were saved in global variables. If the value is the
            same as the previous value, it will then convert those integer RGB
            values to hex and then put them into strings, which are concatenated
            and then sent serially to the PIC. If the values do not match the
            global variables, we set the current values to the global variables
            and begin the scan again. The requirement for the values to be
            matched allows for greater accuracy in which color is output.
            <br><br>
            <h4>PWM and Servo Control</h4>
            For this project, we utilized a singular timer with four different output compare units using it as a
            source. The four output compare units were wired to RB9, RB5, RB2, and RA2, each generating a PWM signal for
            a different servo. Servos were initialized to a closed state based on how they had been mounted. Once ready
            to dispense the dye, we used the values from the CMYK conversion as timing for how long each servo should be
            open. We scaled the values, so the maximum time a servo could be open was one second and the minimum was ten
            milliseconds. To have each servo remain in a given position, we found that setting the corresponding duty
            cycle to zero worked. We were then able to target each servo individually, modifying the period and
            corresponding duty cycle. We created the timings by having the servo thread yield for the specified time as
            shown below.
            <br>

            <pre>
              generate_period2 = (int)(((20.0 + 0.85) / 32.0) * 40000);
              pwm_on_time3 = (int)((0.85 / 32.0) * 40000);
              WritePeriod2(generate_period2);
              SetDCOC3PWM(pwm_on_time3);
              PT_YIELD_TIME_msec(10*error_m);
            </pre>

            Additionally, to have the timer count for larger intervals, we
            increased the prescaler in the timer declaration.

            <pre>
              OpenTimer2(T2_ON | T2_SOURCE_INT | T2_PS_1_32, generate_period2);
            </pre>

            <h4>Serial Communication</h4>

            In order to implement two lines of serial communication on the PIC --
            one to the computer for user input, and one to the Arduino to read the
            color sensor values -- we referenced the instructions on <a
              href="http://people.ece.cornell.edu/land/courses/ece4760/PIC32/index_UART.html">the class webpage.</a>

            Using Protothreads 1.3.3, we used the main serial channel to communicate with the PC and the auxiliary
            serial channel to communicate with the Arduino. For the serial PC communication, we used a UART to USB
            serial cable, connecting RA1 to RX, RB10 to TX, and grounding both. Similarly, for the arduino, we connected
            RB13 to the arduino RX and RB7 to the arduino TX.

            To transmit and receive from the PIC32, we used the serial buffers for the main and auxiliary channels. To
            write, using the main as an example:
            <br><br>
            <code>sprintf(PT_send_buffer, “hex #”);
            PT_SPAWN(pt, &pt_DMA_output, PT_DMA_PutSerialBuffer(&pt_DMA_output));</code>
            <br><br>
            To read, using auxiliary as an example:
            <br><br>
            <code>PT_SPAWN(pt, &pt_input_aux, PT_GetSerialBuffer_aux(&pt_input_aux));
            sscanf(PT_term_buffer_aux, “%s”, &hex_value);</code>
            <br><br>
            Please note that <code>PT_term_buffer_aux</code> is not originally defined in <code>pt_1_3_3.h.</code> This
            is something we defined
            based off of how <code></code>PT_term_buffer</code> was written.

            <br><br>
            <h4>Feedback Control</h4>
            We opted to do user prompted feedback control due to the inconsistency
            of the color sensor. By prompting users if (1) they wanted to adjust t
            he color and (2) to hold a sample up to the color sensor, we could
            obtain a more accurate reading without having to handle the color
            sensor being splashed (if we had placed the sensor close to the
            mixture) and reflectance of the color sensor’s LED (if we had used
            a clear container for the mixture).

            <br><br>
            To perform feedback, we wrote a simple ready/request protocol, where
            the Arduino would initialize and waiting for a request from the
            PIC32. When a user requested a color adjustment, we would then write
            a request to the auxiliary serial buffer and then wait for Arduino.
            The Arduino would then read to color sensor, convert the value to hex,
            and write that to the serial buffer before going back to its ready state.
            The PIC32 would then use HEX to CMYK conversion and compare the sensor
            values with the expected color values.

            <br><br>
            We again prompted the user, displaying both the observed and expected
            values, asking the user if they wanted to proceed with an adjustment.
            If yes, then we observed which of the expected color channels (CMY)
            were zero, because at most two are nonzero at any one time, and
            calculated additive adjustments for the nonzero channels as shown below.
            For the black channel, we only checked if the observed color was too
            bright, and then added more pigment if so. We then used the adjustment
            values to control the servo timings to dispense the appropriate amount
            of pigment for the adjustment.
            <br><br>
            <pre>
              if(final_c &lt; 5){ 
                if(y2 &lt; final_y){
                  error_y = final_y - y2;
                  }
                if(m2 &lt; final_m){
                  error_m=final_m - m2;
                }
              }
            </pre>

            <br><br>

            <h4>Aside: Color Sensor as Input</h4>
            Although this did not make our final demo code due to time
            constraints, we did write code to use the color sensor as an input
            option in addition to the terminal hex value. The idea behind this
            was to be able to replicate the colors of specific objects. The code
            functioned similarly to the user prompted feedback control in that
            the system would ask the user whether they would want to input a hex
            code or scan an object. This version was not included in our demo
            due to inaccuracy of the color sensor on objects with reflective
            surfaces and variable ambient lighting conditions.

          </p>
        </div>
      </div>
    </div>
  </section>
  <section id="results" class="bg-light">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <p class="lead">
            <h2>Results of Design</h2>
            The final results of the project may be seen in our demo video:
            <br>
            <center>
              <iframe width="560" height="315" src="https://www.youtube.com/embed/Y1xkPBjU8A4" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
            </center>
            <br><br>
            <h4>Speed</h4>
            The servos’ response to the user’s hex input is very fast, which you
            can see here (since it was too fast to be caught in the demo):
            <br><br>
            <center>
              <iframe width="560" height="315" src="https://www.youtube.com/embed/07r_sOD8Pi0" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
            </center>
            <br>
            The scanning, on the other hand, takes a variable amount of time.
            In order to maintain accuracy, the arduino will only send color data
            if the color sensor detects the same RGB values twice in a row. While
            this generally means that the scan will be accurate, it also means
            that slight physical movements of the color sensor or surrounding
            light will cause the scanning to take a longer amount of time.

            <br><br>
            <h4>Accuracy</h4>
            <br><br>
            <h4>Safety</h4>
            Our project was safe on its own -- but a project involving lots of
            liquid in an electronics lab has the capability for mild disaster.
            In order to ensure that the liquid stayed away from electricity, we
            were careful about where we placed liquid. We always kept our project
            and any liquid in a Rubbermaid container so that any spills would be
            contained.
            <br><br>
            <h4>Usability</h4>
            Our project by nature is accessible to people with colorblindness,
            because it is not necessary to distinguish colors visually in order
            to create a certain color.
            <br><br>
            The physical design of our project is somewhat inaccessible.
            Loading and removing the cup to catch ink, filling the containers,
            and ensuring the tubes don’t splatter everywhere requires dexterity
            that some people may not have. Even with small hands, interchanging
            the cups resulted in getting ink on one’s hands. To improve this in
            future iterations, we would put more thought into the mechanical
            design to be open and secure.

          </p>
        </div>
      </div>
    </div>
  </section>


  <section id="conclusions">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">

          <p class="lead">
            <h2>Conclusions</h2>
            We were unsure that our project would actually function, as our
            initial plans changed and we ended up having to improvise a lot of
            aspects of the project. However, we were able to create an ink mixer
            that could successfully mix colors based on a hex input and respond
            successfully to open-loop feedback -- in that sense, our project met
            expectations.
            <br><br>
            We were forced to be adaptive with our design, since we initially
            proposed creating a paint mixer. However, we realized that the
            viscosity of paint presented many more mechanical challenges in
            dispensing and mixing the paint -- which were not our technical
            focus (as we were more interested in the color feedback), had a
            larger propensity for failure (due to a lot of moving parts), and
            were pushing the budget limit -- so we switched to working with inks.
            In the future, with more time to figure out a mechanical system, it
            would be fun to create a paint mixer -- but for the time given, it
            was more practical to create an ink mixer.
            <br><br>
            One issue we ran into was a lack of “true” primary colors. The inks
            we used were cheap printer inks, and their corresponding cyan, magenta,
            and yellow were not true to the actual cyan, magenta, and yellow colors.
            <br><br>

            <br><br>
            In the future, we’d like to either 1) purchase better inks or 2) somehow
            compensate for the error in our color conversion. We considered trying
            to figure out how to convert from RGB to the slightly inaccurate CMYK,
            but did not do so due to time constraints -- and our results were
            approximately correct, anyway. We’d also like to do more tests with
            more highly concentrated colors, since we watered down our inks, and
            with more absorbent paper (like watercolor paper).
            <br><br>
            We were proactive in creating a mechanical design early, with the
            hopes of filling it nicely and having a polished product. After we
            changed designs, however, we had to improvise a lot and had a somewhat
            disheveled-looking end result. If we were to do it over again, we would
            ideally have knowledge of our full design before creating the model,
            so that we could make one that actually fit our mechanical design and
            was easily usable by the user.
            <br><br>
            In a similar aesthetic vein, we would also clean up the PuTTY output
            to be more visually appealing, or perhaps create a GUI that would
            allow the user to pick their color on a visual color picker rather
            than just entering the hex code.

          </p>
        </div>
      </div>
    </div>
  </section>

  <section id="references" class="bg-light">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h3>References</h3>
          <p class="lead">
            <ol>
              <li> <a href="https://youtu.be/eLfm1pP-2vs">Our Youtube video</a></li>
              <li> <a href="https://www.youtube.com/watch?v=0vfuOW1tsX0"> Alonso Martinez’ Hand-Crafted Robots </a></li>
              <li> <a href="https://www.youtube.com/watch?v=okFoKJK_N3w"> Blossom Robot</a> </li>
              <li> <a href="https://picamera.readthedocs.io/en/release-1.13/recipes1.html">Picamera Library Docs</a>
              </li>
              <li> <a
                  href="https://www.pyimagesearch.com/2015/07/27/installing-opencv-3-0-for-both-python-2-7-and-python-3-on-your-raspberry-pi-2/">
                  Full installation guide for opencv </a></li>
              <li> <a href="https://bluedot.readthedocs.io/en/latest/btcommapi.html">Bluedot Comm API</a></li>
              <li> ECE 5725, Spring 2019, Lab 2 Week 2 Handout: Lab2_Spring2019_v3, pg. 11-14</li>
              <li> <a
                  href="https://courses.ece.cornell.edu/ece5990/ECE5990_Fall15_FinalProjects/Andre_Heil/ece5990_final_report/avh34_jr986.html">Face
                  Recognition System</a></li>
              <li> <a href="https://learn.adafruit.com/neopixels-on-raspberry-pi/overview">Neopixel Raspberry Pi
                  Tutorial</a></li>
            </ol>
          </p>
        </div>
      </div>
    </div>
  </section>



  <section id="work">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h3>Work Distribution</h3>
          <p class="lead">
            During the beginning of the lab, both Henri and Michael worked on the project’s synthesis and
            problem-solving together, often pair-programming.
            As time constraints wore on the ability for both members of the team to come in at the same time (and
            necessitated rushing to finish), responsibilities were split based on who could come in more.
            Henri focused more on the GUI and NeoPixels and worked some on the facial recognition, while Michael focused
            more on facial recognition and making the communication protocol between Pis work.
            <br><br>
            On the lab report, Henri and Michael wrote about the parts they worked the most on while collaborating on
            the general project-related sections and editing each other’s work.
            Henri organized the information on the web page.
          </p>
        </div>
      </div>
    </div>
  </section>

  <section id="parts" class="bg-light">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h3>Parts List</h3>
          <p class="lead">
            We used some components from lab that were exempt from our budget:
            <br><br>
            <table style="border:1px solid black; width:100%">
              <tr>
                <th><b>Lab Components Used</b></th>
                <th><b>Price</b></th>
              </tr>
              <tr>
                <td>Raspberry Pi 3</td>
                <td>$25</td>
              </tr>
              <tr>
                <td>piTFT screen</td>
                <td>$25</td>
              </tr>
              <tr>
                <td>Expansion cable</td>
                <td>$4</td>
              </tr>
              <tr>
                <td>Case</td>
                <td>$4</td>
              </tr>
              <tr>
                <td>Assembled Lab 3 Robot
                  <ul>
                    <li>2 continuous rotation servos</li>
                    <li>2 wheels</li>
                    <li>2 motor brackets</li>
                    <li>Lower deck</li>
                    <li>Upper deck</li>
                    <li>Small proto-board</li>
                    <li>Caster assembly</li>
                    <li>Assorted fasteners</li>
                  </ul>
                </td>
                <td>$4</td>
              </tr>
              <tr>
                <td><b>Total</b></td>
                <td><b>$58ish</b></td>
              </tr>
            </table>
            <br><br>
            In addition to the components utilized from lab, we purchased or borrowed components that factored into our
            budget allocated ($100)
            <br><br>
            <table style="border:1px solid black; width:100%">
              <tr>
                <th><b>Component Purchased</b></th>
                <th><b>Quantity</b></th>
                <th><b>Vendor</b></th>
                <th><b>Price</b></th>
              </tr>
              <tr>
                <td>Raspberry Pi Camera Board v2 - 8 Megapixels</td>
                <td>1</td>
                <td>Adafruit</td>
                <td>$35</td>
              </tr>
              <tr>
                <td>Raspberry Pi 3</td>
                <td>1</td>
                <td>Professor Skovira</td>
                <td>$25</td>
              </tr>
              <tr>
                <td>Chicken shell</td>
                <td>1</td>
                <td>Walmart</td>
                <td>$5</td>
              </tr>
              <tr>
                <td><b>Total</b></td>
                <td></td>
                <td></td>
                <td><b>$65</b></td>
              </tr>
            </table>
            <br><br>
            We were well within our budget, although we did buy some parts (like the NeoPixels) which we did not use on
            our final robot.
          </p>
        </div>
      </div>
    </div>
  </section>

  <section id="future">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h3>Code Appendix</h3>
          <p class="lead">
            The only files necessary to run the project are:
            <br>
            <b>coopyServer.py</b>, the server/base station code, and <br>

            <b>coopyBot.py</b>, the robot/image processing code
            <br><br>
            Please note, the devices must still be manually paired for bluetooth.
            <br>
            <h4 id=coopyBot> coopyBot.py </h4>
            <pre><code>
                ###############################################################################
                #                                                                             #
                # original file:    4_multi_core.py                                           #
                #                                                                             #
                # authors: Andre Heil  - avh34                                                #
                #          Jingyao Ren - jr386                                                #
                #                                                                             #
                # date:    December 1st 2015                                                  #
                #                                                                             #
                # edited by: Michael Rivera  - mr858										  #
                #            Henri Clarke - hxc2											  #
                #                                                                             #
                # edit date: May 9th 2019                                                     #
                #                                                                             #
                # original brief:   This file uses multicore processing to track your face.   #
                #                   This is similar to 1_single_core.py except now we utilize #
                #                   all four cores to create a more fluid video.              #
                #                                                                             #
                ###############################################################################
                
                
                ### Imports ###################################################################
                
                from picamera.array import PiRGBArray
                from picamera import PiCamera
                from functools import partial
                
                import socket
                import sys
                
                import cv2
                import os
                import time
                import numpy as np
                
                from bluedot.btcomm import BluetoothClient
                from signal import pause
                
                import RPi.GPIO as GPIO
                
                ### Setup #####################################################################
                
                resX = 320
                resY = 240
                
                cx = resX / 2
                cy = resY / 2
                
                # Setup the camera
                camera = PiCamera()
                camera.resolution = ( resX, resY )
                camera.framerate = 60
                
                # Use this as our output
                rawCapture = PiRGBArray( camera, size=( resX, resY ) )
                
                # The face cascade file to be used
                face_cascade = cv2.CascadeClassifier('/home/pi/opencv-2.4.9/data/lbpcascades/lbpcascade_frontalface.xml')
                
                t_start = time.time()
                fps = 0
                
                mode = -1
                ipServer = None
                
                # Bluetooth
                def data_received(data):
                    global mode
                    global ipServer
                    global pl
                    global pr
                    if data == "d" and mode == 5: # Transition from wait state to writing
                        mode = 1
                    elif data == 'h': # Wiggle
                        mode = 2
                    elif data == 'r': # Drive state
                        mode = 3
                    elif data == 'c': # Write state
                        mode = 1
                    elif data == 'b': # Home state
                        mode = 0
                        pl.ChangeDutyCycle(0)
                        pr.ChangeDutyCycle(0)
                    elif data == 'stop': # Stop driving
                        pl.ChangeDutyCycle(0)
                        pr.ChangeDutyCycle(0)
                    elif mode == 3: # Compute direction of driving
                        deg = float(data)
                        uptimer = 0.0
                        uptimel = 0.0
                        if deg > 90:
                            uptimer = 1.3
                            uptimel = 1.7 - .4 * ((float(deg) - 90) / 90)
                            print('q2: ' + str(uptimel) +', ' + str(uptimer))
                        elif deg > 0:
                            uptimel = 1.7
                            uptimer = 1.7 - .4 * ((float(deg) ) / 90)
                            print('q1: ' + str(uptimel) +', ' + str(uptimer))
                        elif deg > - 90:
                            uptimer = 1.7
                            uptimel = 1.3 + .4 * ((float(deg) + 90) / 90)
                            print('q4: ' + str(uptimel) +', ' + str(uptimer))
                        else:
                            uptimel = 1.3
                            uptimer = 1.3 + .4 * ((float(deg) + 180) / 90)
                            print('q3: ' + str(uptimel) +', ' + str(uptimer))
                        pl.ChangeDutyCycle(dcOf(uptimel))
                        pl.ChangeFrequency(freqOf(uptimel))
                        pr.ChangeDutyCycle(dcOf(uptimer))
                        pr.ChangeFrequency(freqOf(uptimer))
                    elif mode == -1: # Obtain ip for socket
                        ipServer = data
                        mode = 0
                    print(data)
                
                # Polling Bluetooth connection
                conn = True
                client = None
                while(conn):
                    conn = False
                    try:
                        client = BluetoothClient("B8:27:EB:9A:90:25",data_received)
                    except:
                        conn = True
                
                
                ### Helper Functions ##########################################################
                
                YOLK = 250, 165, 0
                LIGHTBLUE = 160, 200, 255
                
                # Obtain faces information from classifier
                def get_faces(img):
                    gray = cv2.cvtColor( img, cv2.COLOR_BGR2GRAY )
                    faces = face_cascade.detectMultiScale( gray )
                
                    return faces
                
                # Draw face information onto image and rotate servos
                def draw_frame(img, faces):
                    global fps
                    global time_t
                    global pr
                    global pl
                    global mode
                    wx = 0
                    wc = 0
                    # Draw a rectangle around every face
                    for ( x, y, w, h ) in faces:
                        cv2.rectangle( img, ( x, y ),( x + w, y + h ), YOLK, 2 )
                        cv2.putText(img, "frend no." + str( len( faces ) ), ( x, y ), cv2.FONT_HERSHEY_SIMPLEX, 0.5, LIGHTBLUE, 2 )
                        wx += x + x + w
                        wc += 2
                    if mode == 0 or mode == 1 or mode == 5:
                        if wc == 0:
                            pl.ChangeDutyCycle(0)
                            pr.ChangeDutyCycle(0)
                        else:
                            upT = 1.5 + .04 * ((float(wx/wc) - 160)/(160))
                            updateServos(upT)
                
                    # Calculate and show the FPS
                    fps = fps + 1
                    sfps = fps / (time.time() - t_start)
                    cv2.putText(img, "FPS : " + str( int( sfps ) ), ( 10, 10 ), cv2.FONT_HERSHEY_SIMPLEX, 0.5, ( 0, 0, 255 ), 2 )
                
                    return img
                
                ### Servo Setup ##########################################################
                
                GPIO.setmode(GPIO.BCM)
                GPIO.setup(12,GPIO.OUT)
                GPIO.setup(26,GPIO.OUT)
                GPIO.setup(5,GPIO.IN,pull_up_down=GPIO.PUD_UP)
                
                isRunning = True
                
                def myButton(channel):
                    global isRunning
                    isRunning = False
                
                GPIO.add_event_detect(5,GPIO.FALLING,callback=myButton,bouncetime=300)
                
                pl = GPIO.PWM(12,46.5)
                pr = GPIO.PWM(26,46.5)
                pr.start(0)
                pl.start(0)
                
                def freqOf(uptime):
                    return (1.0 / ((20+uptime) * .001))
                def dcOf(uptime):
                    return (100 * uptime / (20 + uptime))
                def updateServos(uptime):
                    pl.ChangeDutyCycle(dcOf(uptime))
                    pl.ChangeFrequency(freqOf(uptime))
                    pr.ChangeDutyCycle(dcOf(uptime))
                    pr.ChangeFrequency(freqOf(uptime))
                
                
                
                ### Main ######################################################################
                
                shift = -1
                
                for frame in camera.capture_continuous( rawCapture, format="bgr", use_video_port=True ):
                    image = frame.array
                    faces = get_faces(image)
                    dframe = draw_frame(image, faces)
                    rawCapture.truncate( 0 )
                    if not isRunning:
                        break
                    if mode == 1: # Write state
                        client.send("w")
                        s = socket.socket()
                        cv2.imwrite("test.png",dframe) # Save image locally
                        s.connect((ipServer,9999))
                        f = open('test.png','rb') # Read local image to socket
                        l = f.read(1024)
                        while(l):
                            s.send(l)
                            l = f.read(1024)
                        f.close()
                        s.close()
                        mode = 5 # Switch to wait state
                        print(dframe[0][0])
                    if mode == 2: # Wiggle
                        updateServos(1.5 + shift * .2)
                        shift = shift * -1
                
                GPIO.cleanup()
                </code></pre>



            <h4 id=coopyServer> coopyServer.py </h4>
            <pre><code>
                # Henri Clarke (hxc2), Michael Rivera (mr858)
                # Wednesday lab
                # Final Project

                import pygame
                from pygame.locals import * #for event MOUSE variables
                import os
                import RPi.GPIO as GPIO
                import time
                import random

                import socket
                import sys
                import re
                from subprocess import check_output

                import math

                import cv2

                from bluedot.btcomm import BluetoothServer
                from signal import pause

                # ENVIRONMENT VARIABLES: uncomment for piTFT
                #os.putenv('SDL_VIDEODRIVER', 'fbcon')
                #os.putenv('SDL_FBDEV', '/dev/fb1')
                #os.putenv('SDL_MOUSEDRV', 'TSLIB')
                #os.putenv('SDL_MOUSEDEV', '/dev/input/touchscreen')


                # GPIO SETUP
                GPIO.setmode(GPIO.BCM)
                GPIO.setup(17, GPIO.IN, pull_up_down=GPIO.PUD_UP)

                isRunning = True


                # GPIO / BUTTONS
                def myButton(channel):
                    global isRunning
                    isRunning = False

                GPIO.add_event_detect(17, GPIO.FALLING, callback=myButton, bouncetime = 300)


                # PYGAME SETUP
                pygame.init()
                pygame.mouse.set_visible(True) # set false when on piTFT
                WHITE = 255, 255, 255
                BLACK = 0, 0, 0

                RED = 255, 0, 0
                YOLK = 250, 165, 0

                GREEN = 0, 255, 0

                BLUE = 0, 0, 255
                LIGHTBLUE = 160, 200, 255

                rgb = [RED, GREEN, BLUE]
                screen = pygame.display.set_mode((320, 240))
                center_button_loc = (160,60)
                screen_center = 160, 120

                my_font = pygame.font.Font(None, 20)
                home_buttons = {'Friend':(80, 60), 'Camera':(160,60), 'Drive':(240,60)}


                start = time.time()

                i = 0
                j = 0
                state = 0


                # CLOCK SETUP
                clockvar = pygame.time.Clock() #initialization of clock
                global framerate
                framerate = 60


                size = width, height = 320, 240

                # IMAGE SETUP
                chikin = pygame.image.load("../thechikin.png")
                chikinrect = chikin.get_rect()
                chikinrect = chikinrect.move([120,130])

                surf = None
                buff = None

                # Bluetooth and sockets
                s = socket.socket()
                s.bind(('0.0.0.0',9999))
                s.listen(1)

                def data_received(data):
                    global s
                    global surf
                    global buff
                    global state
                    if data == "w": # Upon receiving a new image
                        print(data)
                        sc, address = s.accept() # Open socket
                        print(address)
                        f=open('test.png','wb') # Write file locally
                        l=sc.recv(1024)
                        while(l):
                            f.write(l)
                            l = sc.recv(1024)
                        f.close()
                        sc.close()
                        surf = pygame.image.load("test.png") # Load local image for piTFT
                        buff = surf.get_rect()
                        buff.left = 0
                        buff.top = 0
                        if state == 2: # If in camera display mode
                            server.send('d')

                # When client connects, send ip
                def when_client_connects():
                    temp = check_output(['hostname','-I'])
                    server.send(re.sub('\n','',temp))

                server = BluetoothServer(data_received_callback=data_received,when_client_connects=when_client_connects)


                #WHILE PROGRAM IS RUNNING
                while isRunning:
                    clockvar.tick(framerate) #manual framerate control so we can edit it
                    screen.fill(LIGHTBLUE) #erase the work space
                    if state == 0 or state == 1:

                        screen.blit(chikin, chikinrect)

                        if state == 0: #start screen
                            # render start button
                            text_surface = my_font.render("Start", True, WHITE)
                            rect = text_surface.get_rect(center=center_button_loc)
                            bkgd_rect = pygame.draw.rect(screen, YOLK, (rect.left-15, rect.top-15, rect.width+30, rect.height+30))

                            screen.blit(text_surface,rect)
                            for event in pygame.event.get(): # event handling
                                if((event.type is MOUSEBUTTONDOWN)):
                                    pos = pygame.mouse.get_pos()
                                    i,j = pos
                                    print ("touch at "+ str((i,j)))
                                elif(event.type is MOUSEBUTTONUP):
                                    pos = pygame.mouse.get_pos()
                                    x,y = pos
                                    if y > 35 and y < 75:
                                        if x > 130 and x < 190:
                                            print "start"
                                            state = 1

                        if state == 1: # coopy is ON
                            for my_text, text_pos in home_buttons.items(): # home button rendering

                                text_surface = my_font.render(my_text, True, WHITE)

                                rect = text_surface.get_rect(center=text_pos)
                                bkgd_rect = pygame.draw.rect(screen, YOLK, (rect.left-15, rect.top-15, rect.width+30, rect.height+30))
                                screen.blit(text_surface,rect)
                            for event in pygame.event.get(): # event handling
                                if((event.type is MOUSEBUTTONDOWN)):
                                    pos = pygame.mouse.get_pos()
                                    i,j = pos
                                    state = 1
                                    print ("touch at "+ str((i,j)))
                                elif(event.type is MOUSEBUTTONUP):
                                    pos = pygame.mouse.get_pos()
                                    x,y = pos
                                    if y > 35 and y < 75: #{'Friend':(80, 60), 'Camera':(160,60), 'Drive':(240,60)}
                                        if x > 50 and x < 110: # friend
                                            print "heart"
                                            server.send('h')
                                        elif x > 130 and x < 190: # camera
                                            print "camera"
                                            server.send('c')
                                            state = 2
                                        elif x > 210 and x < 270: # drive
                                            print "drive"
                                            server.send('r')
                                            state = 3
                    if state == 2: # Camera display mode
                        if surf != None:
                            screen.blit(surf,buff) # Load new image to screen
                        text_surface = my_font.render("Back", True, WHITE)
                        rect = text_surface.get_rect(center=(30,30))
                        bkgd_rect = pygame.draw.rect(screen, YOLK, (rect.left-15, rect.top-15, rect.width+30, rect.height+30))

                        screen.blit(text_surface,rect)
                        for event in pygame.event.get(): # event handling
                            if((event.type is MOUSEBUTTONDOWN)):
                                pos = pygame.mouse.get_pos()
                                i,j = pos
                                print ("touch at "+ str((i,j)))
                            elif(event.type is MOUSEBUTTONUP):
                                pos = pygame.mouse.get_pos()
                                x,y = pos
                                if y > 5 and y < 50:
                                    if x > 0 and x < 60:
                                        print "back"
                                        server.send('b')
                                        state = 1
                    if state == 3: # DRIVE MODE

                        # Graphics
                        pygame.draw.circle(screen, YOLK, screen_center, 100, 20)
                        title_top_surface = my_font.render("Press to drive", True, WHITE)
                        title_top_rect = title_top_surface.get_rect(center=(160, 70))
                        title_bottom_surface = my_font.render("in any direction!", True, WHITE)
                        title_bottom_rect = title_bottom_surface.get_rect(center = (160, 90))
                        text_surface = my_font.render("Back", True, WHITE)
                        rect = text_surface.get_rect(center=(160,140))
                        bkgd_rect = pygame.draw.rect(screen, YOLK, (rect.left-15, rect.top-15, rect.width+30, rect.height+30))

                        screen.blit(text_surface,rect)
                        screen.blit(title_top_surface, title_top_rect)
                        screen.blit(title_bottom_surface, title_bottom_rect)


                        for event in pygame.event.get(): # event handling
                            if((event.type is MOUSEBUTTONDOWN)):
                                pos = pygame.mouse.get_pos()
                                i,j = pos
                                deg = math.atan2(-(j-120),i-160) / math.pi*180
                                server.send(str(deg)) # send the angular information of finger press
                            elif(event.type is MOUSEBUTTONUP):
                                pos = pygame.mouse.get_pos()
                                x,y = pos
                                if y > 95 and y < 145:
                                    if x > 130 and x < 190:
                                        print "back"
                                        server.send('b')
                                        state = 1
                                else:
                                    server.send('stop') # lifted finger



                    pygame.display.flip()

                s.close()
                </code></pre>
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- Footer -->
  <footer class="py-5 bg-dark">
    <div class="container">
      <p class="m-0 text-center text-white">Copyright &copy; 2019</p>
    </div>
    <!-- /.container -->
  </footer>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Custom JavaScript for this theme -->
  <script src="js/scrolling-nav.js"></script>

</body>

</html>